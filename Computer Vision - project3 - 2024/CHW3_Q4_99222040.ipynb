{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NFWCZjean-w-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiheadAttentionEinsum(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads):\n",
        "        super(MultiheadAttentionEinsum, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embedding_dim // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.k_linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.v_linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.fc_out = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "\n",
        "        Q = self.q_linear(query)\n",
        "        K = self.k_linear(key)\n",
        "        V = self.v_linear(value)\n",
        "\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "\n",
        "        scores = torch.einsum(\"nqhd,nkhd->nhqk\", Q, K) / (self.head_dim ** 0.5)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = torch.einsum(\"nhqk,nkhd->nqhd\", attn, V).reshape(batch_size, -1, self.num_heads * self.head_dim)\n",
        "\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.multihead_attention = MultiheadAttentionEinsum(embed_dim=embedding_dim, num_heads=num_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, embedding_dim)\n",
        "        )\n",
        "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        attn_output = self.multihead_attention(x, x, x)[0]\n",
        "        x = attn_output + residual\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        residual = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = x + residual\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "QvDu51LRoL3R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, num_classes, patch_size, embedding_dim, num_heads, num_layers):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.patch_embedding = nn.Conv2d(3, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 14 * 14 + 1, embedding_dim))\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.patch_embedding(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = torch.cat((x, self.positional_encoding.repeat(batch_size, 1, 1)), dim=1)\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pXdbHiSYoTzb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_classes = 10\n",
        "patch_size = 16\n",
        "embedding_dim = 128\n",
        "num_heads = 8\n",
        "num_layers = 3\n",
        "\n",
        "# CIFAR-10 dataset preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgb67adwoYAZ",
        "outputId": "eb3dc6f9-b7ee-4e3f-99ee-f71fc983713d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12532544.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model\n",
        "model = VisionTransformer(num_classes, patch_size, embedding_dim, num_heads, num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7sRJ3OJoc02",
        "outputId": "371c0e00-9826-4aee-945e-cc1f78a7637b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/782], Loss: 2.1960\n",
            "Epoch [1/10], Step [200/782], Loss: 2.0369\n",
            "Epoch [1/10], Step [300/782], Loss: 1.9783\n",
            "Epoch [1/10], Step [400/782], Loss: 2.0499\n",
            "Epoch [1/10], Step [500/782], Loss: 2.1027\n",
            "Epoch [1/10], Step [600/782], Loss: 1.9756\n",
            "Epoch [1/10], Step [700/782], Loss: 1.9870\n",
            "Epoch [2/10], Step [100/782], Loss: 2.0334\n",
            "Epoch [2/10], Step [200/782], Loss: 2.0514\n",
            "Epoch [2/10], Step [300/782], Loss: 1.8165\n",
            "Epoch [2/10], Step [400/782], Loss: 1.7043\n",
            "Epoch [2/10], Step [500/782], Loss: 1.8695\n",
            "Epoch [2/10], Step [600/782], Loss: 1.9248\n",
            "Epoch [2/10], Step [700/782], Loss: 1.8271\n",
            "Epoch [3/10], Step [100/782], Loss: 2.0409\n",
            "Epoch [3/10], Step [200/782], Loss: 1.6809\n",
            "Epoch [3/10], Step [300/782], Loss: 1.7645\n",
            "Epoch [3/10], Step [400/782], Loss: 1.7958\n",
            "Epoch [3/10], Step [500/782], Loss: 1.7031\n",
            "Epoch [3/10], Step [600/782], Loss: 1.5427\n",
            "Epoch [3/10], Step [700/782], Loss: 1.5633\n",
            "Epoch [4/10], Step [100/782], Loss: 1.5802\n",
            "Epoch [4/10], Step [200/782], Loss: 1.6690\n",
            "Epoch [4/10], Step [300/782], Loss: 1.6498\n",
            "Epoch [4/10], Step [400/782], Loss: 1.4828\n",
            "Epoch [4/10], Step [500/782], Loss: 1.6572\n",
            "Epoch [4/10], Step [600/782], Loss: 1.5550\n",
            "Epoch [4/10], Step [700/782], Loss: 1.6384\n",
            "Epoch [5/10], Step [100/782], Loss: 1.5908\n",
            "Epoch [5/10], Step [200/782], Loss: 1.3385\n",
            "Epoch [5/10], Step [300/782], Loss: 1.4435\n",
            "Epoch [5/10], Step [400/782], Loss: 1.5398\n",
            "Epoch [5/10], Step [500/782], Loss: 1.6796\n",
            "Epoch [5/10], Step [600/782], Loss: 1.7247\n",
            "Epoch [5/10], Step [700/782], Loss: 1.4584\n",
            "Epoch [6/10], Step [100/782], Loss: 1.3962\n",
            "Epoch [6/10], Step [200/782], Loss: 1.3568\n",
            "Epoch [6/10], Step [300/782], Loss: 1.5249\n",
            "Epoch [6/10], Step [400/782], Loss: 1.5431\n",
            "Epoch [6/10], Step [500/782], Loss: 1.4586\n",
            "Epoch [6/10], Step [600/782], Loss: 1.6585\n",
            "Epoch [6/10], Step [700/782], Loss: 1.2678\n",
            "Epoch [7/10], Step [100/782], Loss: 1.3869\n",
            "Epoch [7/10], Step [200/782], Loss: 1.5330\n",
            "Epoch [7/10], Step [300/782], Loss: 1.2848\n",
            "Epoch [7/10], Step [400/782], Loss: 1.3457\n",
            "Epoch [7/10], Step [500/782], Loss: 1.4472\n",
            "Epoch [7/10], Step [600/782], Loss: 1.4185\n",
            "Epoch [7/10], Step [700/782], Loss: 1.2532\n",
            "Epoch [8/10], Step [100/782], Loss: 1.3772\n",
            "Epoch [8/10], Step [200/782], Loss: 1.4199\n",
            "Epoch [8/10], Step [300/782], Loss: 1.3722\n",
            "Epoch [8/10], Step [400/782], Loss: 1.4572\n",
            "Epoch [8/10], Step [500/782], Loss: 1.6232\n",
            "Epoch [8/10], Step [600/782], Loss: 1.4589\n",
            "Epoch [8/10], Step [700/782], Loss: 1.6519\n",
            "Epoch [9/10], Step [100/782], Loss: 1.3363\n",
            "Epoch [9/10], Step [200/782], Loss: 1.7338\n",
            "Epoch [9/10], Step [300/782], Loss: 1.3563\n",
            "Epoch [9/10], Step [400/782], Loss: 1.1609\n",
            "Epoch [9/10], Step [500/782], Loss: 1.5062\n",
            "Epoch [9/10], Step [600/782], Loss: 1.3635\n",
            "Epoch [9/10], Step [700/782], Loss: 1.4299\n",
            "Epoch [10/10], Step [100/782], Loss: 1.1747\n",
            "Epoch [10/10], Step [200/782], Loss: 1.4285\n",
            "Epoch [10/10], Step [300/782], Loss: 1.3487\n",
            "Epoch [10/10], Step [400/782], Loss: 1.5253\n",
            "Epoch [10/10], Step [500/782], Loss: 1.0541\n",
            "Epoch [10/10], Step [600/782], Loss: 1.1159\n",
            "Epoch [10/10], Step [700/782], Loss: 1.2656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy of the model on the {total} test images: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9_VfGMdoqca",
        "outputId": "49a0b7fe-5f9f-4b07-e4de-63ad95621996"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 54.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#new hyperparameters for faster training\n",
        "num_epochs = 5\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "fast_model = VisionTransformer(num_classes, patch_size, embedding_dim, num_heads, num_layers).to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(fast_model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    fast_model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "\n",
        "        outputs = fast_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "fast_model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = fast_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy of the fast model on the {total} test images: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdhQLsOWowHg",
        "outputId": "a5946de5-0d68-4eb4-c5ea-aed7bca4ba1b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/782], Loss: 2.0736\n",
            "Epoch [1/5], Step [200/782], Loss: 2.0383\n",
            "Epoch [1/5], Step [300/782], Loss: 1.9827\n",
            "Epoch [1/5], Step [400/782], Loss: 1.9877\n",
            "Epoch [1/5], Step [500/782], Loss: 2.0787\n",
            "Epoch [1/5], Step [600/782], Loss: 1.8285\n",
            "Epoch [1/5], Step [700/782], Loss: 1.9361\n",
            "Epoch [2/5], Step [100/782], Loss: 1.9641\n",
            "Epoch [2/5], Step [200/782], Loss: 1.7718\n",
            "Epoch [2/5], Step [300/782], Loss: 1.8143\n",
            "Epoch [2/5], Step [400/782], Loss: 1.7858\n",
            "Epoch [2/5], Step [500/782], Loss: 1.6520\n",
            "Epoch [2/5], Step [600/782], Loss: 1.6235\n",
            "Epoch [2/5], Step [700/782], Loss: 1.5844\n",
            "Epoch [3/5], Step [100/782], Loss: 1.6691\n",
            "Epoch [3/5], Step [200/782], Loss: 1.7881\n",
            "Epoch [3/5], Step [300/782], Loss: 1.6308\n",
            "Epoch [3/5], Step [400/782], Loss: 1.6871\n",
            "Epoch [3/5], Step [500/782], Loss: 1.5572\n",
            "Epoch [3/5], Step [600/782], Loss: 1.5729\n",
            "Epoch [3/5], Step [700/782], Loss: 1.5953\n",
            "Epoch [4/5], Step [100/782], Loss: 1.6539\n",
            "Epoch [4/5], Step [200/782], Loss: 1.4734\n",
            "Epoch [4/5], Step [300/782], Loss: 1.4029\n",
            "Epoch [4/5], Step [400/782], Loss: 1.5152\n",
            "Epoch [4/5], Step [500/782], Loss: 1.6383\n",
            "Epoch [4/5], Step [600/782], Loss: 1.5002\n",
            "Epoch [4/5], Step [700/782], Loss: 1.3492\n",
            "Epoch [5/5], Step [100/782], Loss: 1.3444\n",
            "Epoch [5/5], Step [200/782], Loss: 1.4159\n",
            "Epoch [5/5], Step [300/782], Loss: 1.4971\n",
            "Epoch [5/5], Step [400/782], Loss: 1.3589\n",
            "Epoch [5/5], Step [500/782], Loss: 1.4301\n",
            "Epoch [5/5], Step [600/782], Loss: 1.4986\n",
            "Epoch [5/5], Step [700/782], Loss: 1.5363\n",
            "Test Accuracy of the fast model on the 10000 test images: 50.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "it seems that these parameters don't have better results than before. so I will test new hyperparameters in the below"
      ],
      "metadata": {
        "id": "dkd0XGGVrdN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#new hyperparameters\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "learning_rate = 0.0001\n",
        "embedding_dim = 192\n",
        "num_heads = 12\n",
        "num_layers = 5\n",
        "\n",
        "better_model = VisionTransformer(num_classes, patch_size, embedding_dim, num_heads, num_layers).to(device)\n",
        "\n",
        "optimizer = optim.Adam(better_model.parameters(), lr=learning_rate)\n",
        "\n",
        "total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    better_model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "\n",
        "        outputs = better_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "better_model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = better_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy of the better model on the {total} test images: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EvTUIDtowsO",
        "outputId": "a377740b-3906-4586-91b3-ba1db81564b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/782], Loss: 2.1389\n",
            "Epoch [1/10], Step [200/782], Loss: 2.0677\n",
            "Epoch [1/10], Step [300/782], Loss: 2.0865\n",
            "Epoch [1/10], Step [400/782], Loss: 1.7972\n",
            "Epoch [1/10], Step [500/782], Loss: 1.8876\n",
            "Epoch [1/10], Step [600/782], Loss: 1.7640\n",
            "Epoch [1/10], Step [700/782], Loss: 1.9183\n",
            "Epoch [2/10], Step [100/782], Loss: 1.8415\n",
            "Epoch [2/10], Step [200/782], Loss: 1.7543\n",
            "Epoch [2/10], Step [300/782], Loss: 1.6258\n",
            "Epoch [2/10], Step [400/782], Loss: 1.7255\n",
            "Epoch [2/10], Step [500/782], Loss: 1.6265\n",
            "Epoch [2/10], Step [600/782], Loss: 1.6652\n",
            "Epoch [2/10], Step [700/782], Loss: 1.6803\n",
            "Epoch [3/10], Step [100/782], Loss: 1.6722\n",
            "Epoch [3/10], Step [200/782], Loss: 1.5482\n",
            "Epoch [3/10], Step [300/782], Loss: 1.6027\n",
            "Epoch [3/10], Step [400/782], Loss: 1.5421\n",
            "Epoch [3/10], Step [500/782], Loss: 1.4406\n",
            "Epoch [3/10], Step [600/782], Loss: 1.4171\n",
            "Epoch [3/10], Step [700/782], Loss: 1.5027\n",
            "Epoch [4/10], Step [100/782], Loss: 1.4334\n",
            "Epoch [4/10], Step [200/782], Loss: 1.5702\n",
            "Epoch [4/10], Step [300/782], Loss: 1.4917\n",
            "Epoch [4/10], Step [400/782], Loss: 1.3470\n",
            "Epoch [4/10], Step [500/782], Loss: 1.6217\n",
            "Epoch [4/10], Step [600/782], Loss: 1.4451\n",
            "Epoch [4/10], Step [700/782], Loss: 1.4263\n",
            "Epoch [5/10], Step [100/782], Loss: 1.3947\n",
            "Epoch [5/10], Step [200/782], Loss: 1.3212\n",
            "Epoch [5/10], Step [300/782], Loss: 1.5805\n",
            "Epoch [5/10], Step [400/782], Loss: 1.2308\n",
            "Epoch [5/10], Step [500/782], Loss: 1.3095\n",
            "Epoch [5/10], Step [600/782], Loss: 1.2714\n",
            "Epoch [5/10], Step [700/782], Loss: 1.4720\n",
            "Epoch [6/10], Step [100/782], Loss: 1.2297\n",
            "Epoch [6/10], Step [200/782], Loss: 1.4156\n",
            "Epoch [6/10], Step [300/782], Loss: 1.1944\n",
            "Epoch [6/10], Step [400/782], Loss: 1.2250\n",
            "Epoch [6/10], Step [500/782], Loss: 1.4540\n",
            "Epoch [6/10], Step [600/782], Loss: 1.1921\n",
            "Epoch [6/10], Step [700/782], Loss: 1.3580\n",
            "Epoch [7/10], Step [100/782], Loss: 1.3806\n",
            "Epoch [7/10], Step [200/782], Loss: 1.1593\n",
            "Epoch [7/10], Step [300/782], Loss: 1.2239\n",
            "Epoch [7/10], Step [400/782], Loss: 1.4109\n",
            "Epoch [7/10], Step [500/782], Loss: 1.1875\n",
            "Epoch [7/10], Step [600/782], Loss: 1.0423\n",
            "Epoch [7/10], Step [700/782], Loss: 1.2035\n",
            "Epoch [8/10], Step [100/782], Loss: 1.2568\n",
            "Epoch [8/10], Step [200/782], Loss: 1.1394\n",
            "Epoch [8/10], Step [300/782], Loss: 1.1321\n",
            "Epoch [8/10], Step [400/782], Loss: 1.1534\n",
            "Epoch [8/10], Step [500/782], Loss: 1.0034\n",
            "Epoch [8/10], Step [600/782], Loss: 1.2135\n",
            "Epoch [8/10], Step [700/782], Loss: 1.1827\n",
            "Epoch [9/10], Step [100/782], Loss: 1.2471\n",
            "Epoch [9/10], Step [200/782], Loss: 1.1049\n",
            "Epoch [9/10], Step [300/782], Loss: 1.1028\n",
            "Epoch [9/10], Step [400/782], Loss: 1.0738\n",
            "Epoch [9/10], Step [500/782], Loss: 1.2152\n",
            "Epoch [9/10], Step [600/782], Loss: 1.0406\n",
            "Epoch [9/10], Step [700/782], Loss: 1.1066\n",
            "Epoch [10/10], Step [100/782], Loss: 1.1543\n",
            "Epoch [10/10], Step [200/782], Loss: 1.1242\n",
            "Epoch [10/10], Step [300/782], Loss: 0.9834\n",
            "Epoch [10/10], Step [400/782], Loss: 1.1212\n",
            "Epoch [10/10], Step [500/782], Loss: 1.1292\n",
            "Epoch [10/10], Step [600/782], Loss: 1.0335\n",
            "Epoch [10/10], Step [700/782], Loss: 1.1747\n",
            "Test Accuracy of the better model on the 10000 test images: 60.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vgg\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "#Hyperparameters\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_classes = 10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "vgg_model = models.vgg16(pretrained=True)\n",
        "\n",
        "\n",
        "vgg_model.classifier[6] = nn.Linear(vgg_model.classifier[6].in_features, num_classes)\n",
        "\n",
        "\n",
        "vgg_model = vgg_model.to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vgg_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RW16T1no1Jk",
        "outputId": "c468073e-ad66-4208-88e2-f19896b3fea8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 72.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    vgg_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "\n",
        "        outputs = vgg_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_steps}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A6FsZfOqfz2",
        "outputId": "a212dc41-205f-4bb7-f9f8-6d817abff22b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/782], Loss: 2.0554\n",
            "Epoch [1/10], Step [200/782], Loss: 2.0801\n",
            "Epoch [1/10], Step [300/782], Loss: 1.9316\n",
            "Epoch [1/10], Step [400/782], Loss: 1.8572\n",
            "Epoch [1/10], Step [500/782], Loss: 2.0636\n",
            "Epoch [1/10], Step [600/782], Loss: 1.6279\n",
            "Epoch [1/10], Step [700/782], Loss: 1.8516\n",
            "Epoch [2/10], Step [100/782], Loss: 1.6406\n",
            "Epoch [2/10], Step [200/782], Loss: 1.5471\n",
            "Epoch [2/10], Step [300/782], Loss: 1.5558\n",
            "Epoch [2/10], Step [400/782], Loss: 1.5739\n",
            "Epoch [2/10], Step [500/782], Loss: 1.2151\n",
            "Epoch [2/10], Step [600/782], Loss: 1.5280\n",
            "Epoch [2/10], Step [700/782], Loss: 1.3711\n",
            "Epoch [3/10], Step [100/782], Loss: 0.9659\n",
            "Epoch [3/10], Step [200/782], Loss: 1.3221\n",
            "Epoch [3/10], Step [300/782], Loss: 1.3090\n",
            "Epoch [3/10], Step [400/782], Loss: 0.9485\n",
            "Epoch [3/10], Step [500/782], Loss: 1.0455\n",
            "Epoch [3/10], Step [600/782], Loss: 0.9109\n",
            "Epoch [3/10], Step [700/782], Loss: 1.0114\n",
            "Epoch [4/10], Step [100/782], Loss: 0.8991\n",
            "Epoch [4/10], Step [200/782], Loss: 0.8269\n",
            "Epoch [4/10], Step [300/782], Loss: 1.1703\n",
            "Epoch [4/10], Step [400/782], Loss: 1.4343\n",
            "Epoch [4/10], Step [500/782], Loss: 1.0066\n",
            "Epoch [4/10], Step [600/782], Loss: 1.2133\n",
            "Epoch [4/10], Step [700/782], Loss: 1.2616\n",
            "Epoch [5/10], Step [100/782], Loss: 0.9244\n",
            "Epoch [5/10], Step [200/782], Loss: 0.9452\n",
            "Epoch [5/10], Step [300/782], Loss: 0.6321\n",
            "Epoch [5/10], Step [400/782], Loss: 1.1801\n",
            "Epoch [5/10], Step [500/782], Loss: 0.7963\n",
            "Epoch [5/10], Step [600/782], Loss: 0.9436\n",
            "Epoch [5/10], Step [700/782], Loss: 0.8969\n",
            "Epoch [6/10], Step [100/782], Loss: 0.8981\n",
            "Epoch [6/10], Step [200/782], Loss: 0.8022\n",
            "Epoch [6/10], Step [300/782], Loss: 0.9537\n",
            "Epoch [6/10], Step [400/782], Loss: 0.8145\n",
            "Epoch [6/10], Step [500/782], Loss: 0.3712\n",
            "Epoch [6/10], Step [600/782], Loss: 0.4955\n",
            "Epoch [6/10], Step [700/782], Loss: 0.5541\n",
            "Epoch [7/10], Step [100/782], Loss: 0.7666\n",
            "Epoch [7/10], Step [200/782], Loss: 0.6759\n",
            "Epoch [7/10], Step [300/782], Loss: 0.8305\n",
            "Epoch [7/10], Step [400/782], Loss: 0.7238\n",
            "Epoch [7/10], Step [500/782], Loss: 0.6616\n",
            "Epoch [7/10], Step [600/782], Loss: 0.4276\n",
            "Epoch [7/10], Step [700/782], Loss: 0.9416\n",
            "Epoch [8/10], Step [100/782], Loss: 0.8848\n",
            "Epoch [8/10], Step [200/782], Loss: 0.9763\n",
            "Epoch [8/10], Step [300/782], Loss: 0.6510\n",
            "Epoch [8/10], Step [400/782], Loss: 0.8977\n",
            "Epoch [8/10], Step [500/782], Loss: 0.6570\n",
            "Epoch [8/10], Step [600/782], Loss: 0.5865\n",
            "Epoch [8/10], Step [700/782], Loss: 0.7381\n",
            "Epoch [9/10], Step [100/782], Loss: 0.6913\n",
            "Epoch [9/10], Step [200/782], Loss: 0.7794\n",
            "Epoch [9/10], Step [300/782], Loss: 0.9778\n",
            "Epoch [9/10], Step [400/782], Loss: 0.4697\n",
            "Epoch [9/10], Step [500/782], Loss: 0.7790\n",
            "Epoch [9/10], Step [600/782], Loss: 0.6615\n",
            "Epoch [9/10], Step [700/782], Loss: 0.9711\n",
            "Epoch [10/10], Step [100/782], Loss: 0.6714\n",
            "Epoch [10/10], Step [200/782], Loss: 0.5662\n",
            "Epoch [10/10], Step [300/782], Loss: 0.7181\n",
            "Epoch [10/10], Step [400/782], Loss: 0.5819\n",
            "Epoch [10/10], Step [500/782], Loss: 0.6022\n",
            "Epoch [10/10], Step [600/782], Loss: 0.6840\n",
            "Epoch [10/10], Step [700/782], Loss: 0.4707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = vgg_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy of the model on the {total} test images: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spvURRCzqkAh",
        "outputId": "cebf1f7e-2fc8-47dd-fe54-f713df2782c8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 78.90%\n"
          ]
        }
      ]
    }
  ]
}